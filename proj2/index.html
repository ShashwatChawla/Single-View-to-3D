<!DOCTYPE html>
<h1 id="single-view-to-3d">Single View to 3D</h1>
<html>
<head>
    <title>Single View to 3D Reconstruction</title>
    <style>
        #toc {
            background: #f8f9fa;
            padding: 20px;
            margin-bottom: 30px;
            border-radius: 5px;
        }
        #toc ul {
            list-style-type: none;
            padding-left: 20px;
        }
        #toc li {
            margin: 5px 0;
        }
        #toc a {
            text-decoration: none;
            color: #0645ad;
        }
        #toc a:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <div id="toc">
        <h2>Table of Contents</h2>
        
                    <li><a href="#1-exploring-loss-functions">1. Exploring loss functions</a>
                        <ul>
                            <li><a href="#11-fitting-a-voxel-grid">1.1. Fitting a voxel grid</a></li>
                            <li><a href="#12-fitting-a-point-cloud">1.2. Fitting a point cloud</a></li>
                            <li><a href="#13-fitting-a-mesh">1.3. Fitting a mesh</a></li>
                        </ul>
                    </li>
                </ul>
            </li>
            <li><a href="#2-reconstructing-3d-from-single-view">2. Reconstructing 3D from single view</a>
                <ul>
                    <li><a href="#21-image-to-voxel-grid">2.1. Image to voxel grid</a></li>
                    <li><a href="#22-image-to-point-cloud">2.2. Image to point cloud</a></li>
                    <li><a href="#23-image-to-mesh">2.3. Image to mesh</a></li>
                    <li><a href="#24-quantitative-comparisions">2.4. Quantitative comparisons</a>
                        <ul>
                            <li><a href="#f1-score-for-predicted-voxel">F1 score for voxel</a></li>
                            <li><a href="#f1-score-for-predicted-point-cloud">F1 score for point cloud</a></li>
                            <li><a href="#f1-score-for-predicted-mesh">F1 score for mesh</a></li>
                        </ul>
                    </li>
                    <li><a href="#25-analyse-effects-of-hyperparams-variations">2.5. Hyperparameter analysis</a></li>
                    <li><a href="#26-interpret-your-model">2.6. Model interpretation</a></li>
                </ul>
            </li>
            <li><a href="#3-exploring-other-architectures--datasets">3. Other architectures & datasets</a>
                <ul>
                    <li><a href="#33-extended-dataset-for-training">3.3 Extended dataset</a></li>
                </ul>
            </li>
        </ul>
    </div>

<h2 id="1-exploring-loss-functions">1. Exploring loss functions</h2>
<hr>
<h2 id="11-fitting-a-voxel-grid">1.1. Fitting a voxel grid</h2>
<p>Left is ground-truth, while right is fitted voxel</p>
<p><img src="results/fit_data-vox.gif" alt="img" title="Fitting Voxel"></p>
<h2 id="12-fitting-a-point-cloud">1.2. Fitting a point cloud</h2>
<p>Left is ground-truth, while right is fitted point-cloud</p>
<p><img src="results/fit_data-point.gif" alt="img" title="Point Cloud"></p>
<h2 id="13-fitting-a-mesh">1.3. Fitting a mesh</h2>
<p>Left is ground-truth, while right is fitted mesh</p>
<p><img src="results/fit_data-mesh.gif" alt="img" title="Mesh"></p>
<h1 id="2-reconstructing-3d-from-single-view">2. Reconstructing 3D from single view</h1>
<hr>
<h2 id="21-image-to-voxel-grid">2.1. Image to voxel grid</h2>
<p>From left to right: RGB image, ground-truth rendered mesh, predicted rendered mesh</p>
<p><img src="results/vox/100_vox.gif" alt="img"></p>
<p><img src="results/vox/200_vox.gif" alt="img"></p>
<p><img src="results/vox/400_vox.gif" alt="img"></p>
<h2 id="22-image-to-point-cloud">2.2. Image to point cloud</h2>
<p>From left to right: RGB image, ground-truth rendered mesh, predicted rendered pointcloud</p>
<p><img src="results/point/100_point.gif" alt="img"></p>
<p><img src="results/point/200_point.gif" alt="img"></p>
<p><img src="results/point/400_point.gif" alt="img"></p>
<h2 id="23-image-to-mesh">2.3. Image to mesh</h2>
<p>From left to right: RGB image, ground-truth rendered mesh, predicted rendered mesh</p>
<p><img src="results/mesh/100_mesh.gif" alt="img"></p>
<p><img src="results/mesh/200_mesh.gif" alt="img"></p>
<p><img src="results/mesh/400_mesh.gif" alt="img"></p>
<h2 id="24-quantitative-comparisions">2.4. Quantitative comparisions</h2>
<h4 id="f1-score-for-predicted-voxel">F1 score for predicted voxel</h4>
<p>Final avg score: 51.1</p>
<p><img src="results/eval_vox.png" alt="img"></p>
<h4 id="f1-score-for-predicted-point-cloud">F1 score for predicted point cloud</h4>
<p>Final avg score: 75.65</p>
<p><img src="results/eval_point.png" alt="img"></p>
<h4 id="f1-score-for-predicted-mesh">F1 score for predicted mesh</h4>
<p>Final avg score: 72.2</p>
<p><img src="results/eval_mesh.png" alt="img"></p>
<p><strong>Explanation:</strong></p>
<ul>
<li><strong>Point Clouds:</strong> They are a sparse representation of 3D geometry, capturing key surface points without needing to fill an entire volume. Because they focus on essential geometric features, they’re easier to align with ground truth, leading to a highest F1 score.</li>
<li><strong>Meshes:</strong> These represent surfaces with vertices and faces, providing more structure than point clouds but still focusing on the object&#39;s boundary. While more detailed, they can introduce errors in surface connectivity or topology, thus the F1 score is lower than point-cloud.</li>
<li><strong>Voxels:</strong> Voxel grids discretize 3D space into small cubes, requiring dense, volumetric predictions. This introduces ambiguity and high false positives/negatives, especially with limited input views, leading to the lowest the F1 score.</li>
</ul>
<h2 id="25-analyse-effects-of-hyperparams-variations">2.5. Analyse effects of hyperparams variations</h2>
<p>I varied the <em>&#39;n_points&#39;</em> hyperparameter and analysed the the effects</p>
<table>
<thead>
<tr>
<th>n_points</th>
<th>F1 Score</th>
<th>F1 plot</th>
<th>Gif@300 Image</th>
</tr>
</thead>
<tbody><tr>
<td>512</td>
<td>70.65</td>
<td><img src="results/point/_512_eval_point.png" alt="img"></td>
<td><img src="results/point/_512_300_point.gif" alt="img"></td>
</tr>
<tr>
<td>1000</td>
<td>75.66</td>
<td><img src="results/eval_point.png" alt="img"></td>
<td><img src="results/point/_1000_300_point.gif" alt="img"></td>
</tr>
<tr>
<td>2048</td>
<td>80.83</td>
<td><img src="results/point/_2048_eval_point.png" alt="img"></td>
<td><img src="results/point/_2048_300_point.gif" alt="img"></td>
</tr>
</tbody></table>
<p><strong>Analysis:</strong> As the number of points increases, the average F1 score improves because more samples help the model converge closer to the optimal result. However, this also increases GPU memory usage. The 1000-point model offers a balanced tradeoff between achieving a high F1 score and managing memory consumption.</p>
<h2 id="26-interpret-your-model">2.6. Interpret your model</h2>
<p>To better understand precision, I visualized the nearest neighbors (k-NN) from the ground truth mesh to the predicted point cloud. Similarly, to gain insight into recall, I visualized the nearest neighbors from the predicted point cloud to the ground truth mesh. The GIFs below illustrate these visualizations:</p>
<p>From left to right:</p>
<ol>
<li>Original Mesh</li>
<li>Predicted Point Cloud</li>
<li>Nearest Neighbors (Ground Truth → Predicted)</li>
<li>Nearest Neighbors (Predicted → Ground Truth)</li>
</ol>
<p>The color gradient represents the distance error:</p>
<ul>
<li><strong>Blue:</strong> Points with low error (closer match)</li>
<li><strong>Red:</strong> Points with high error (larger distance)</li>
</ul>
<p><img src="results/interpret/0_point.gif" alt="img"></p>
<p><img src="results/interpret/400_point.gif" alt="img"></p>
<h1 id="3-exploring-other-architectures--datasets">3. Exploring other architectures / datasets</h1>
<hr>
<h2 id="33-extended-dataset-for-training">3.3 Extended dataset for training</h2>
<p>I trained the point-cloud decoder on the extended r2n2_shapenet_dataset model</p>
<h5 id="visualizations"><strong>Visualizations</strong></h5>
<p>From left to right: original Image, original Mesh, predicted point cloud</p>
<p>As shown in the third example, for the same chair class, the predicted point cloud appears more geometrically aligned with the original mesh. This suggests that the model learns the characteristics of a chair more effectively, likely due to the increased diversity in the training data.</p>
<p><img src="results/point/extended/0_point.gif" alt="img"></p>
<p><img src="results/point/extended/500_point.gif" alt="img"></p>
<p><img src="results/point/extended/800_point.gif" alt="img"></p>
<p><strong>Failure Example:</strong> The predicted mesh does not align with the original mesh. This discrepancy may stem from a lack of similar example images in the dataset, or it could indicate the need for a more complex model to better capture intricate geometric features.<img src="results/point/extended/700_point.gif" alt="img"></p>
<h5 id="model-comparison--training-on-one-class-vs-training-on-three-classes">Model comparison  &quot;training on one class&quot; VS &quot;training on three classes&quot;</h5>
<table>
<thead>
<tr>
<th>Training Set</th>
<th>F1 Score</th>
<th>F1 plot</th>
</tr>
</thead>
<tbody><tr>
<td>One class</td>
<td>75.66</td>
<td><img src="results/eval_point.png" alt="img"></td>
</tr>
<tr>
<td>Three classes</td>
<td>85.73</td>
<td><img src="results/point/extended/eval_point.png" alt="img"></td>
</tr>
</tbody></table>
<p>The average F1 score significantly increases to 85.73, indicating that the model learns more effectively about the object&#39;s general shape, resulting in a better fit. Greater class diversity enhances the model&#39;s ability to understand and represent each object more accurately.</p>

